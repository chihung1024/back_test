# â”€â”€ update_data.pyï¼ˆå¢é‡æ›´æ–°ç‰ˆï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json, time, requests, pandas as pd
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import yfinance as yf

# â”€â”€â”€ è³‡æ–™å¤¾è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR     = Path("data")
PRICES_DIR   = DATA_DIR / "prices"
PARQUET_FILE = DATA_DIR / "prices.parquet.gz"
JSON_FILE    = DATA_DIR / "preprocessed_data.json"
MAX_WORKERS  = 20

DATA_DIR.mkdir(exist_ok=True)
PRICES_DIR.mkdir(exist_ok=True)

# â”€â”€â”€ 1. å–å¾—æŒ‡æ•¸æˆåˆ†è‚¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sp500_official() -> list[str]:
    """ç›´æ¥è§£æ S&P å®˜ç¶²é é¢éš±è—çš„ indexMembers JSONã€‚"""
    try:
        html = requests.get(
            "https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview",
            timeout=10
        ).text
        i = html.find("indexMembers")
        if i == -1: return []
        l = html.find("[", i)
        r = html.find("]", l) + 1
        return [m["symbol"] for m in json.loads(html[l:r])]
    except Exception:
        return []

def nasdaq_official() -> list[str]:
    """ä½¿ç”¨ Nasdaq å®˜æ–¹ JSON APIã€‚"""
    try:
        hdr  = {"User-Agent": "Mozilla/5.0"}
        rows = requests.get(
            "https://api.nasdaq.com/api/quote/NDX/constituents",
            headers=hdr, timeout=10
        ).json()["data"]["rows"]
        return [r["symbol"] for r in rows]
    except Exception:
        return []

def fmp_etf_components(etf: str) -> list[str]:
    """ä»¥ FMP API ç•¶å‚™æ´ä¾†æºã€‚"""
    key = os.getenv("FMP_TOKEN")
    if not key: return []
    try:
        url  = f"https://financialmodelingprep.com/api/v3/etf-holder/{etf}?apikey={key}"
        rows = requests.get(url, timeout=10).json()
        return [
            row.get("symbol") or row.get("asset")
            for row in rows if isinstance(row, dict)
        ]
    except Exception:
        return []

def etf_holdings(etf: str) -> list[str]:
    """æœ€å¾Œå‚™æ´ï¼šç›´æ¥çœ‹ ETF æˆä»½ã€‚"""
    try:
        hold = yf.Ticker(etf).holdings
        return hold["symbol"].tolist() if hold is not None else []
    except Exception:
        return []

def wiki_sp500() -> list[str]:
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        return pd.read_html(url)[0]["Symbol"].str.replace(".", "-").tolist()
    except Exception:
        return []

def wiki_nasdaq100() -> list[str]:
    try:
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        return pd.read_html(url)[4]["Ticker"].tolist()
    except Exception:
        return []

def get_sp500() -> list[str]:
    for fn in (sp500_official, lambda: fmp_etf_components("VOO")):
        res = fn()
        if res: return res
    return etf_holdings("VOO") or wiki_sp500()

def get_nasdaq100() -> list[str]:
    for fn in (nasdaq_official, lambda: fmp_etf_components("QQQ")):
        res = fn()
        if res: return res
    return etf_holdings("QQQ") or wiki_nasdaq100()

# â”€â”€â”€ 2. åŸºæœ¬é¢èˆ‡æ­·å²åƒ¹æ ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASIC_EXTRA = [
    "priceToBook", "priceToSalesTrailing12Months", "ebitdaMargins",
    "grossMargins", "operatingMargins", "debtToEquity"
]

def fetch_fundamentals(ticker: str):
    """æŠ“å–å–®æª”åŸºæœ¬é¢ï¼Œå¤±æ•—å›å‚³ Noneã€‚"""
    try:
        info = yf.Ticker(ticker).info
        if not info.get("marketCap"):
            return None
        row = {
            "ticker": ticker,
            "marketCap":        info.get("marketCap"),
            "sector":           info.get("sector"),
            "trailingPE":       info.get("trailingPE"),
            "forwardPE":        info.get("forwardPE"),
            "dividendYield":    info.get("dividendYield"),
            "returnOnEquity":   info.get("returnOnEquity"),
            "revenueGrowth":    info.get("revenueGrowth"),
            "earningsGrowth":   info.get("earningsGrowth"),
        }
        for k in BASIC_EXTRA:
            row[k] = info.get(k)
        return row
    except Exception:
        return None

def fetch_history(
    ticker: str,
    start_date: str,
    max_retries: int = 3,
    pause_sec: float = 1.0
):
    """
    ä¸‹è¼‰å–®æª”æ­·å²åƒ¹æ ¼ï¼ˆå¾ start_date èµ·ï¼‰ã€‚
    æˆåŠŸï¼šå›å‚³ (ticker, True) ä¸”åœ¨ data/prices ç”Ÿæˆ .csv.gz
    å¤±æ•—ï¼šé‡è©¦å¾Œå›å‚³ (ticker, False)
    """
    for attempt in range(1, max_retries + 1):
        try:
            df = yf.download(
                ticker,
                start=start_date,          # â† ä¾å¢é‡é‚è¼¯å‚³å…¥
                progress=False,
                auto_adjust=True
            )
            if df.empty or "Close" not in df.columns:
                raise ValueError("empty frame or no Close column")
            out = df[["Close"]].copy()
            out.index.name = "Date"
            out.to_csv(
                PRICES_DIR / f"{ticker}.csv.gz",
                index_label="Date",
                compression="gzip"
            )
            return ticker, True
        except Exception:
            if attempt == max_retries:
                return ticker, False
            time.sleep(pause_sec)

# â”€â”€â”€ 3. ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    t0 = time.time()

    # 3-0 è¨ˆç®—å¢é‡èµ·å§‹æ—¥
    existing_df = None
    start_date  = "1990-01-01"               # é è¨­å…¨é‡
    if PARQUET_FILE.exists():
        print(f"â„¹ï¸ ç™¼ç¾ç¾æœ‰åƒ¹æ ¼æª”ï¼š{PARQUET_FILE}")
        existing_df = pd.read_parquet(PARQUET_FILE)
        if not existing_df.empty:
            last_date  = existing_df.index.max()
            start_date = (last_date + pd.Timedelta(days=1)).strftime("%Y-%m-%d")
            print(f"ğŸ“… åªä¸‹è¼‰ {start_date} ä¹‹å¾Œçš„æ–°è³‡æ–™")

    # 3-1 å–å¾—æˆä»½è‚¡
    sp500_set = set(get_sp500())
    ndx_set   = set(get_nasdaq100())
    tickers   = sorted(sp500_set | ndx_set)
    if not tickers:
        print("âŒ ç„¡æ³•å–å¾—ä»»ä½•æˆä»½è‚¡ï¼ŒçµæŸåŸ·è¡Œ"); return
    print("Total symbols:", len(tickers))

    # 3-2 åŸºæœ¬é¢
    fundamentals = []
    with ThreadPoolExecutor(MAX_WORKERS) as ex:
        jobs = {ex.submit(fetch_fundamentals, t): t for t in tickers}
        for fut in tqdm(as_completed(jobs), total=len(jobs), desc="Fundamentals"):
            data = fut.result()
            if data: fundamentals.append(data)
    for row in fundamentals:
        row["in_sp500"]   = row["ticker"] in sp500_set
        row["in_nasdaq100"] = row["ticker"] in ndx_set

    # 3-3 æ­·å²åƒ¹æ ¼ï¼ˆå¢é‡ï¼‰
    success = set()
    with ThreadPoolExecutor(MAX_WORKERS) as ex:
        jobs = {ex.submit(fetch_history, t, start_date): t for t in tickers}
        for fut in tqdm(as_completed(jobs), total=len(jobs), desc=f"Prices â‰¥ {start_date}"):
            tk, ok = fut.result()
            if ok: success.add(tk)

    # 3-4 åˆä½µæ–°èˆŠåƒ¹æ ¼
    new_frames = []
    for tk in success:
        csv_path = PRICES_DIR / f"{tk}.csv.gz"
        if csv_path.exists():
            df_new_single = pd.read_csv(csv_path, index_col="Date", parse_dates=True)
            if "Close" in df_new_single.columns:
                new_frames.append(df_new_single["Close"].rename(tk))

    if not new_frames:
        print("â„¹ï¸ æ²’æœ‰ä¸‹è¼‰åˆ°ä»»ä½•æ–°åƒ¹æ ¼è³‡æ–™ï¼Œè·³é Parquet æ›´æ–°")
    else:
        new_df = pd.concat(new_frames, axis=1).sort_index()
        if existing_df is not None:
            combined = pd.concat([existing_df, new_df])
            final_df = combined[~combined.index.duplicated(keep="last")]
        else:
            final_df = new_df
        final_df.to_parquet(PARQUET_FILE, compression="gzip")
        print(f"ğŸ’¾ å·²å¯«å…¥åˆä½µå¾Œ Parquetï¼Œç­†æ•¸ï¼š{len(final_df)}")

    # 3-5 åŸºæœ¬é¢è®Šæ›´åµæ¸¬
    new_fund_df = pd.DataFrame(fundamentals).sort_values("ticker").reset_index(drop=True)
    if JSON_FILE.exists():
        old_df = pd.read_json(JSON_FILE, orient="records")
        if new_fund_df.equals(old_df):
            print("â„¹ï¸ åŸºæœ¬é¢ç„¡è®Šå‹•ï¼Œè·³éå¯«æª”")
            print(f"âœ… æ›´æ–°å®Œæˆï¼Œè€—æ™‚ {time.time() - t0:.1f}s"); return
    new_fund_df.to_json(JSON_FILE, orient="records", indent=2)
    print(f"âœ… æ›´æ–°å®Œæˆï¼Œè€—æ™‚ {time.time() - t0:.1f}s")

if __name__ == "__main__":
    main()
