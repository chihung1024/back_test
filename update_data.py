# â”€â”€ update_data.py (æœ€çµ‚ç©©å®šç‰ˆ - ç›´æ¥ä¸‹è¼‰å®˜æ–¹ CSV) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŠŸèƒ½ï¼š
# 1. é€éç›´æ¥ä¸‹è¼‰ iShares å®˜ç¶²çš„ CSV æª”æ¡ˆï¼Œç©©å®šç²å– IWB (Russell 1000 ETF) çš„æŒè‚¡ã€‚
# 2. å¤šåŸ·è¡Œç·’ä¸¦è¡Œä¸‹è¼‰æ­·å²åƒ¹æ ¼èˆ‡åŸºæœ¬é¢æ•¸æ“šã€‚
# 3. å°‡æ‰€æœ‰è‚¡åƒ¹åˆä½µç‚ºå–®ä¸€ã€é«˜æ•ˆçš„ Parquet æª”æ¡ˆã€‚

import os
import json
import time
import pandas as pd
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import yfinance as yf
import requests
from io import StringIO

# --- Configuration ---
DATA_DIR = Path("data")
PRICES_DIR = DATA_DIR / "prices"
PARQUET_FILE = DATA_DIR / "prices.parquet.gz"
JSON_FILE = DATA_DIR / "preprocessed_data.json"
MAX_WORKERS = 20

# --- Ensure Directories Exist ---
DATA_DIR.mkdir(exist_ok=True)
PRICES_DIR.mkdir(exist_ok=True)

def get_russell1000_constituents_from_ishares() -> list[str]:
    """
    ç›´æ¥å¾ iShares (BlackRock) å®˜ç¶²ä¸‹è¼‰ IWB ETF çš„æŒè‚¡ CSV æª”æ¡ˆã€‚
    é€™æ˜¯æœ€ç©©å®šå’Œå®˜æ–¹çš„æ–¹æ³•ã€‚
    """
    try:
        # å½è£æˆç€è¦½å™¨ä»¥é¿å…è¢«é˜»æ“‹
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # iShares æä¾›çš„ IWB æŒè‚¡ CSV ä¸‹è¼‰é€£çµ
        url = "https://www.ishares.com/us/products/239707/ishares-russell-1000-etf/1467271812596.ajax?fileType=csv&fileName=IWB_holdings&dataType=fund"
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()  # å¦‚æœè«‹æ±‚å¤±æ•—å‰‡æ‹‹å‡ºéŒ¯èª¤

        # è®€å– CSV å…§å®¹ï¼Œè·³é iShares CSV æª”æ¡ˆé–‹é ­çš„èªªæ˜æ–‡å­—
        # æˆ‘å€‘é€éå°‹æ‰¾ "Ticker" é€™å€‹è©ä¾†ç¢ºå®šè¡¨æ ¼çš„èµ·å§‹ä½ç½®
        content = response.text
        if 'Ticker' not in content:
            raise ValueError("CSV content does not contain 'Ticker' header.")

        # å°‡å¾ "Ticker" é–‹å§‹çš„å…§å®¹è®€å…¥ pandas DataFrame
        csv_data = StringIO(content[content.find('Ticker'):])
        df = pd.read_csv(csv_data)
        
        # ç¯©é¸æ‰ç¾é‡‘ç­‰éè‚¡ç¥¨è³‡ç”¢
        df_stocks = df[df['Asset Class'] == 'Equity'].copy()
        
        tickers = df_stocks['Ticker'].dropna().unique().tolist()
        
        print(f"âœ… Successfully fetched {len(tickers)} stock tickers from iShares official CSV.")
        return tickers
        
    except Exception as e:
        print(f"ğŸ”´ Failed to download or parse iShares holdings CSV: {e}")
        return []

def fetch_fundamentals(ticker: str):
    """æŠ“å–å–®æª”åŸºæœ¬é¢æ•¸æ“šã€‚"""
    try:
        info = yf.Ticker(ticker).info
        if not info.get("marketCap"): return None
        return { "ticker": ticker, "marketCap": info.get("marketCap"), "sector": info.get("sector"), "trailingPE": info.get("trailingPE"), "forwardPE": info.get("forwardPE"), "dividendYield": info.get("dividendYield"), "returnOnEquity": info.get("returnOnEquity"), "revenueGrowth": info.get("revenueGrowth"), "earningsGrowth": info.get("earningsGrowth"), "priceToBook": info.get("priceToBook"), "priceToSalesTrailing12Months": info.get("priceToSalesTrailing12Months"), "operatingMargins": info.get("operatingMargins"), }
    except Exception:
        return None

def fetch_history(ticker: str, max_retries: int = 3, pause_sec: float = 1.0):
    """ä¸‹è¼‰å–®æª”è‚¡ç¥¨çš„æ­·å²åƒ¹æ ¼ä¸¦å­˜ç‚ºå£“ç¸® CSVã€‚"""
    for _ in range(max_retries):
        try:
            df = yf.download(ticker, start="1990-01-01", progress=False, auto_adjust=True)
            if df.empty or "Close" not in df.columns: raise ValueError("Empty data")
            out = df[["Close"]].copy()
            out.index.name = "Date"
            out.to_csv(PRICES_DIR / f"{ticker}.csv.gz", compression="gzip")
            return ticker, True
        except Exception:
            time.sleep(pause_sec)
    return ticker, False

def main():
    """ä¸»åŸ·è¡Œæµç¨‹"""
    t0 = time.time()

    tickers = get_russell1000_constituents_from_ishares()

    if not tickers:
        print("âŒ Failed to fetch constituents. Aborting update.")
        return

    # --- å¾ŒçºŒæµç¨‹ä¸è®Š ---
    fundamentals, successful_tickers = [], set()
    with ThreadPoolExecutor(MAX_WORKERS) as executor:
        future_to_ticker = {executor.submit(fetch_fundamentals, t): t for t in tickers}
        future_to_ticker.update({executor.submit(fetch_history, t): t for t in tickers})
        for future in tqdm(as_completed(future_to_ticker), total=len(future_to_ticker), desc="Fetching data"):
            result = future.result()
            if isinstance(result, dict) and result:
                fundamentals.append(result)
            elif isinstance(result, tuple) and result[1]:
                successful_tickers.add(result[0])

    print(f"\nFetched fundamentals for {len(fundamentals)} tickers.")
    print(f"Fetched price history for {len(successful_tickers)} tickers.")

    frames = []
    for tk in tqdm(sorted(list(successful_tickers)), desc="Merging prices"):
        file_path = PRICES_DIR / f"{tk}.csv.gz"
        if file_path.exists():
            df = pd.read_csv(file_path, index_col="Date", parse_dates=True)
            if not df.empty:
                frames.append(df["Close"].rename(tk))

    if frames:
        full_df = pd.concat(frames, axis=1).sort_index()
        full_df.to_parquet(PARQUET_FILE, compression="gzip")
        print(f"Successfully merged {len(frames)} tickers into {PARQUET_FILE}")

    if fundamentals:
        new_df = pd.DataFrame(fundamentals).sort_values("ticker").reset_index(drop=True)
        new_df.to_json(JSON_FILE, orient="records", indent=2)
        print(f"Successfully saved fundamental data to {JSON_FILE}")
        
    print(f"âœ… Data update complete. Total time: {time.time() - t0:.1f} seconds.")

if __name__ == "__main__":
    main()
